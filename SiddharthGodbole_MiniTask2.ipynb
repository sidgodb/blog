{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiddharthGodbole_MiniTask2",
      "provenance": [],
      "collapsed_sections": [
        "ATnR9JLmdhYP",
        "8JRck629-Po0",
        "bHC9y9zgcReB",
        "hFnMHhaztMxk",
        "rahuipARxi47"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sidgodb/blog/blob/master/SiddharthGodbole_MiniTask2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JJrBRSM4W0z"
      },
      "source": [
        "# Minitask 2 - ADAMS Next Top Model \n",
        "\n",
        "> Identifying and predicting the star rating after analysing the reviews \n",
        "\n",
        "**Contributors:**\n",
        ">Siddharth Godbole (601552)\n",
        "<br> Toby Chelton (601819)\n",
        "<br>Veronika Simoncikova (601550)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejvW03zcZroj"
      },
      "source": [
        "# Loading the data and packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7Wg9Co2LnRT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f1f5fe7a-357c-4054-b713-dcf54bac81a0"
      },
      "source": [
        "#setup code\n",
        "#main packages from Ex.4\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re #regular expressions\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "\n",
        "#for word tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "# we will need to import some core layers and some utilities\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "## Plotly\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "# Others\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from __future__ import print_function\n",
        "from nltk.stem import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/vnd.plotly.v1+html": "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>",
            "text/html": [
              "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pb3O8r2vByk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "bdea9a58-be86-4559-abb4-297d0a05b58d"
      },
      "source": [
        "#setup code\n",
        "#code to load in data https://medium.freecodecamp.org/how-to-transfer-large-files-to-google-colab-and-remote-jupyter-notebooks-26ca252892fa\n",
        "\n",
        "!pip install pydrive #if needed\n",
        "\n",
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "#the file is public so this works for any Google Drive\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pydrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\u001b[K     |████████████████████████████████| 993kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.9)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.5)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.5)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n",
            "Building wheels for collected packages: pydrive\n",
            "  Building wheel for pydrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built pydrive\n",
            "Installing collected packages: pydrive\n",
            "Successfully installed pydrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGHh24bOxOp0"
      },
      "source": [
        "**Uploading the datasets**\n",
        "We have upload all the training and testing datasets MC2 and MC2test.csv \n",
        "<br> Along with that we are using **Glove** dataset. This Glove dataset has been downloaded and then uploaded back. There are various different versions of Glove that we could use - different sizes and sources. What we have here is 6 billion words each represented as 100 length vector.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD_3B1cIB8nq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "a0ae53c5-eda8-4063-d0e5-5c03f115306a"
      },
      "source": [
        "#setup code\n",
        "#code to tell Colab what to import from Google Drive\n",
        "train = drive.CreateFile({'id': '1ylh9tt89V3nezuOKUxxQMDPb1kizngdP'})\n",
        "train.GetContentFile('MC2.csv')\n",
        "\n",
        "test = drive.CreateFile({'id': '1d9JqCjNb-AXh25Pi-i6Hf94SXsueXF5s'})\n",
        "test.GetContentFile('MC2test.csv')\n",
        "\n",
        "\n",
        "glove = drive.CreateFile({'id': '1fx_E-moM2aUJsdQ4KRC2zRQPE3fxfGEj'})\n",
        "glove.GetContentFile('glove.6B.100d.txt')\n",
        "\n",
        "#setup code\n",
        "#reading in data as a pandas dataframe\n",
        "\n",
        "train = pd.read_csv('MC2.csv')\n",
        "test=pd.read_csv('MC2test.csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0630 08:04:51.322088 139685384861568 __init__.py:44] file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 36, in autodetect\n",
            "    from google.appengine.api import memcache\n",
            "ModuleNotFoundError: No module named 'google.appengine'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 33, in <module>\n",
            "    from oauth2client.contrib.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.contrib.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 37, in <module>\n",
            "    from oauth2client.locked_file import LockedFile\n",
            "ModuleNotFoundError: No module named 'oauth2client.locked_file'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/__init__.py\", line 41, in autodetect\n",
            "    from . import file_cache\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/googleapiclient/discovery_cache/file_cache.py\", line 41, in <module>\n",
            "    'file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth')\n",
            "ImportError: file_cache is unavailable when using oauth2client >= 4.0.0 or google-auth\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kib0DPTTrv-L"
      },
      "source": [
        "# Exploring the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0PvOHafK_Gi"
      },
      "source": [
        "<br> The data given to use includes different fields liked product_id, product_title, star_rating, helpful_votes, total_votes, verified_purchases and ofcourse the review_headline and review_body. \n",
        "<br> Our aim is to predict the star_rating based on the different fields provided to us. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfSErpitsLoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "d4576189-e4aa-4fcc-9ea2-b743bcf76762"
      },
      "source": [
        "train.head(6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>product_id</th>\n",
              "      <th>product_title</th>\n",
              "      <th>star_rating</th>\n",
              "      <th>helpful_votes</th>\n",
              "      <th>total_votes</th>\n",
              "      <th>verified_purchase</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0983797706</td>\n",
              "      <td>Igniting Your True Purpose and Passion: A Busi...</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>This is an inspirational and insightful book t...</td>\n",
              "      <td>This is an inspirational and insightful book t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1423151283</td>\n",
              "      <td>The Duckling Gets a Cookie!? (Pigeon)</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Loved it!</td>\n",
              "      <td>My twins are 3 and they love the pigeon books!...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1445604752</td>\n",
              "      <td>Spitfire Ace of Aces: The Wartime Story of Joh...</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>Engaging Account of the Combat Career of an RA...</td>\n",
              "      <td>Back in 1964, I was introduced to 'Johnnie' Jo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0393057941</td>\n",
              "      <td>The Bread Bible</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "      <td>Great book for beginners</td>\n",
              "      <td>I disagree with those reviews that say this is...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1608322858</td>\n",
              "      <td>Do It Well. Make It Fun.: The Key to Success i...</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>N</td>\n",
              "      <td>Left me wanting more</td>\n",
              "      <td>Reading this book, I hoped to find more about ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>B001FVQAPW</td>\n",
              "      <td>Myth and Christianity: An Inquiry into the Pos...</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>N</td>\n",
              "      <td>A \"SEMI-DIALOGUE\" BETWEEN TWO TOWERING 20TH CE...</td>\n",
              "      <td>Rudolf Bultmann (1884-1976) was a German theol...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  ...                                        review_body\n",
              "0           1  ...  This is an inspirational and insightful book t...\n",
              "1           2  ...  My twins are 3 and they love the pigeon books!...\n",
              "2           3  ...  Back in 1964, I was introduced to 'Johnnie' Jo...\n",
              "3           4  ...  I disagree with those reviews that say this is...\n",
              "4           5  ...  Reading this book, I hoped to find more about ...\n",
              "5           6  ...  Rudolf Bultmann (1884-1976) was a German theol...\n",
              "\n",
              "[6 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_0sWGVDLXwQ"
      },
      "source": [
        "> In the given dataset, the number of reviews receiving 5 star rating is much higher compared to the other ratings. However, we are dealing with a dataset which has a great amount of data and hence, our modeling approach should be able to handle this. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfk7_c4frw0v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "ef238a2f-566d-4a4f-e4d0-6c4d2b83396b"
      },
      "source": [
        "plt.hist(train.star_rating, bins=5,alpha=0.5, color='gold', rwidth=0.7) #density=True if want to see probability of a rating\n",
        "plt.xticks([1.4,2.2,3,3.8,4.6],['*','**', '***','****','*****'])\n",
        "plt.ylabel('Number of ratings')\n",
        "plt.title(\"Ratings by Stars\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "#most reviews have 5 stars"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEICAYAAACJalkVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGpl6C6PdUez"
      },
      "source": [
        "## Data Preparation\n",
        "\n",
        "> Checking the total number of rows, cleaning the text involving processes like removing stop words, lower casing everything, lemmatization and stemming. This is to make the text processing easier. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXaqVHH5VhTY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "29eebd99-8ea6-427b-c3ee-cf8776a4e3cb"
      },
      "source": [
        "train.shape\n",
        "#the dataset has over 1 million rows"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1099209, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1dzYGBzhbNr"
      },
      "source": [
        "There's a great amount of data (over 1 million) -  let's start by just working with a small part of the train set in order to speed up computation, say 10 percent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDJM3hvFatgW"
      },
      "source": [
        "\n",
        "\n",
        "For going ahead with our analyses, we tried doing two approaches : \n",
        "\n",
        "<br> 1. Analysis entirely with review_body.\n",
        "- Although the review_title idea sounds promising, many times people write detailed and descriptive reviews and then give a star_rating. \n",
        "- In this case, analysing the entire review_body would make more sense. \n",
        "\n",
        "<br> 2. Analysing entirely with review_headline \n",
        "- Our understanding in this case was that it has a shorter length and hence computationally less intense. \n",
        "- People normally would have the 'exact' emotion out when writing the review_headline making it easy to link to the star_rating \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxGwLiOFKmsi"
      },
      "source": [
        "#10 percent of data\n",
        "train2 = train.sample(100000,random_state=123,axis=0)\n",
        "\n",
        "train2=train2[[\"star_rating\",\"review_body\"]]\n",
        "train2.review_body=train2.review_body.astype(str)\n",
        "train2.star_rating=train2.star_rating.astype(str)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdQhRlFPy7IZ"
      },
      "source": [
        "Before we do anything, we must set some test data aside"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86om4jB2rZGJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bca3d1d1-397d-43f2-d7d0-419bbc2974a2"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train ,test = train_test_split(train2, test_size=0.30, random_state=9)\n",
        "\n",
        "train.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb8JM9NPzApr"
      },
      "source": [
        "We switch to the conventional notation for feature(s) (=X) and target (=Y):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjvKGererzwL"
      },
      "source": [
        "X=train['review_body']\n",
        "y=train['star_rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDRqaG6vzVu1"
      },
      "source": [
        "**Cleaning data:**\n",
        "<br>The following code is for cleaning the content of each review text that we encounter. This could be applied for 'review_body' or 'review_head' .\n",
        "<br> It removes punctuation (even though in theory this could be informative, especially exclamation marks), stop words, very short words and then some common strings that are unlikely to hold semantic information. \n",
        "<br>\n",
        "When we then test our model at the end, we'll also need to apply this 'clean_text' function on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rv-CZfk2dDVd"
      },
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    ## Remove punctuation\n",
        "    text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text) \n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    ## Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return text\n",
        "# apply the above function to df['text']\n",
        "X= X.map(lambda x: clean_text(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMAcAdefzz5T"
      },
      "source": [
        "Here we represent the score out of 5 as a one-hot encoded array.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjeyOjExf_pP"
      },
      "source": [
        "y=pd.get_dummies(y)\n",
        "y=y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CT7qqStF0WgS"
      },
      "source": [
        "The keras tokenizer is one way to tokenize the review text into a sequence of integers\n",
        "\n",
        "<br>\n",
        "'maxlen' in the pad_sequences function controls how many words we allow to be included in a review. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdoinNzTAbWE"
      },
      "source": [
        "### Create sequence\n",
        "vocabulary_size = 50000\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(sequences, maxlen=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rpe66Osyeubq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc282d81-f3f6-4e8c-8e32-c8b47abcdeef"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgGgWkovvhuR"
      },
      "source": [
        "Splitting the training and test dataset over here. In this case, X_val and y_val are our test datasets which will given as validation data input for keras to test for accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPZWuhsYuniq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "df4de8ed-33fb-44a1-d1cf-535fafa2bee5"
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=9)\n",
        "\n",
        "X_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(21000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATnR9JLmdhYP"
      },
      "source": [
        "## Model Architecture - Review_Body"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFnH_ZB30vAN"
      },
      "source": [
        "Now that our data is cleaned and in the right shape, we start building model architecture (starting with small number of epochs). Our first basic model includes an embedding layer that is transforming the word token integers into a 100-length vector representation, and Keras tokenizer.\n",
        "<br>\n",
        "In this model the embeddings are trained at the same time as everything else.<br>\n",
        "In all following models, we use *Softmax* as an activation function in the last layer, which will assign each review a range of probabilities for each rating, with 5* being the target. <br>\n",
        "Moreover, we use *categorical crossentropy* as a loss function because our goal is non-binary classification. <br>\n",
        "We chose *adam* as the optimizer because it is straightforward to implement, computationally efficient and has little memory requirement"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWPMBWm-gNw1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23063a1a-28d0-4b8a-ee25-ec54ab0bdcf5"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(50000, 100, input_length=100))\n",
        "model.add(LSTM(200, dropout=0.5, recurrent_dropout=0.2)) #originally dropout=0.2, but was hugely overfitting\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "## Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 30, batch_size=3000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 49000 samples, validate on 21000 samples\n",
            "Epoch 1/30\n",
            "49000/49000 [==============================] - 10s 198us/step - loss: 1.3277 - acc: 0.5883 - val_loss: 1.1587 - val_acc: 0.6075\n",
            "Epoch 2/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 1.1149 - acc: 0.6166 - val_loss: 1.0932 - val_acc: 0.6075\n",
            "Epoch 3/30\n",
            "49000/49000 [==============================] - 9s 174us/step - loss: 1.0238 - acc: 0.6205 - val_loss: 1.0042 - val_acc: 0.6143\n",
            "Epoch 4/30\n",
            "49000/49000 [==============================] - 9s 175us/step - loss: 0.9320 - acc: 0.6367 - val_loss: 0.9759 - val_acc: 0.6243\n",
            "Epoch 5/30\n",
            "49000/49000 [==============================] - 9s 174us/step - loss: 0.8860 - acc: 0.6490 - val_loss: 0.9519 - val_acc: 0.6287\n",
            "Epoch 6/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.8459 - acc: 0.6619 - val_loss: 0.9505 - val_acc: 0.6295\n",
            "Epoch 7/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.8147 - acc: 0.6728 - val_loss: 0.9625 - val_acc: 0.6335\n",
            "Epoch 8/30\n",
            "49000/49000 [==============================] - 8s 171us/step - loss: 0.7776 - acc: 0.6879 - val_loss: 0.9534 - val_acc: 0.6341\n",
            "Epoch 9/30\n",
            "49000/49000 [==============================] - 8s 171us/step - loss: 0.7441 - acc: 0.7040 - val_loss: 0.9591 - val_acc: 0.6416\n",
            "Epoch 10/30\n",
            "49000/49000 [==============================] - 8s 170us/step - loss: 0.7026 - acc: 0.7215 - val_loss: 0.9687 - val_acc: 0.6398\n",
            "Epoch 11/30\n",
            "49000/49000 [==============================] - 8s 171us/step - loss: 0.6704 - acc: 0.7371 - val_loss: 0.9986 - val_acc: 0.6426\n",
            "Epoch 12/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.6436 - acc: 0.7496 - val_loss: 1.0247 - val_acc: 0.6359\n",
            "Epoch 13/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.6178 - acc: 0.7622 - val_loss: 1.0218 - val_acc: 0.6190\n",
            "Epoch 14/30\n",
            "49000/49000 [==============================] - 8s 173us/step - loss: 0.6025 - acc: 0.7680 - val_loss: 1.0932 - val_acc: 0.6379\n",
            "Epoch 15/30\n",
            "49000/49000 [==============================] - 8s 173us/step - loss: 0.5735 - acc: 0.7801 - val_loss: 1.0806 - val_acc: 0.6177\n",
            "Epoch 16/30\n",
            "49000/49000 [==============================] - 8s 173us/step - loss: 0.5538 - acc: 0.7909 - val_loss: 1.1166 - val_acc: 0.6283\n",
            "Epoch 17/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.5266 - acc: 0.8006 - val_loss: 1.1465 - val_acc: 0.6258\n",
            "Epoch 18/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.5053 - acc: 0.8115 - val_loss: 1.1825 - val_acc: 0.6199\n",
            "Epoch 19/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4902 - acc: 0.8166 - val_loss: 1.2102 - val_acc: 0.6193\n",
            "Epoch 20/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4722 - acc: 0.8256 - val_loss: 1.2148 - val_acc: 0.6092\n",
            "Epoch 21/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4496 - acc: 0.8354 - val_loss: 1.2754 - val_acc: 0.6140\n",
            "Epoch 22/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4320 - acc: 0.8420 - val_loss: 1.3034 - val_acc: 0.6100\n",
            "Epoch 23/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4204 - acc: 0.8472 - val_loss: 1.3096 - val_acc: 0.6094\n",
            "Epoch 24/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.4075 - acc: 0.8521 - val_loss: 1.4047 - val_acc: 0.6048\n",
            "Epoch 25/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3924 - acc: 0.8596 - val_loss: 1.4279 - val_acc: 0.6190\n",
            "Epoch 26/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3771 - acc: 0.8661 - val_loss: 1.4280 - val_acc: 0.5926\n",
            "Epoch 27/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3615 - acc: 0.8697 - val_loss: 1.4778 - val_acc: 0.6037\n",
            "Epoch 28/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3428 - acc: 0.8793 - val_loss: 1.5141 - val_acc: 0.6030\n",
            "Epoch 29/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3315 - acc: 0.8831 - val_loss: 1.5029 - val_acc: 0.6000\n",
            "Epoch 30/30\n",
            "49000/49000 [==============================] - 8s 172us/step - loss: 0.3191 - acc: 0.8874 - val_loss: 1.5864 - val_acc: 0.6058\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5ad0b30860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XqyB5ct1naB6"
      },
      "source": [
        "#Saving this model\n",
        "\n",
        "model.save('model.h5')\n",
        "model_file = drive.CreateFile({'title' : 'model_1.h5'}) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4JxuQvNc1sP"
      },
      "source": [
        "The model above reached accuracy on validation set around 60%, but is overfitting (performing much better on the in-sample test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0HXXrt71YOu"
      },
      "source": [
        "We also tried adding a few more layers including a pooling layer, 'relu' activation layer and made the network more dense. The results are better than the network built before.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-siF30vkgxdB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "e49217a1-65e6-4e49-d762-7e5471a8fe0e"
      },
      "source": [
        "def create_conv_model():\n",
        "    model_conv = Sequential()\n",
        "    model_conv.add(Embedding(vocabulary_size, 100, input_length=100))\n",
        "    model_conv.add(Dropout(0.2)) #originally dropout=0.2\n",
        "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
        "    model_conv.add(MaxPooling1D(pool_size=5)) #originally pool_size=4\n",
        "    model_conv.add(LSTM(100))\n",
        "    model_conv.add(Dense(5, activation='softmax'))\n",
        "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model_conv\n",
        "model_conv = create_conv_model()\n",
        "model_conv.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 8, batch_size=3000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 49000 samples, validate on 21000 samples\n",
            "Epoch 1/8\n",
            "49000/49000 [==============================] - 4s 86us/step - loss: 1.3346 - acc: 0.5826 - val_loss: 1.1468 - val_acc: 0.6075\n",
            "Epoch 2/8\n",
            "49000/49000 [==============================] - 2s 34us/step - loss: 1.1268 - acc: 0.6166 - val_loss: 1.1239 - val_acc: 0.6075\n",
            "Epoch 3/8\n",
            "49000/49000 [==============================] - 2s 34us/step - loss: 1.0807 - acc: 0.6166 - val_loss: 1.0672 - val_acc: 0.6075\n",
            "Epoch 4/8\n",
            "49000/49000 [==============================] - 2s 34us/step - loss: 0.9988 - acc: 0.6167 - val_loss: 1.0050 - val_acc: 0.6087\n",
            "Epoch 5/8\n",
            "49000/49000 [==============================] - 2s 34us/step - loss: 0.9214 - acc: 0.6313 - val_loss: 0.9691 - val_acc: 0.6194\n",
            "Epoch 6/8\n",
            "49000/49000 [==============================] - 2s 35us/step - loss: 0.8461 - acc: 0.6608 - val_loss: 0.9463 - val_acc: 0.6327\n",
            "Epoch 7/8\n",
            "49000/49000 [==============================] - 2s 35us/step - loss: 0.7827 - acc: 0.6835 - val_loss: 0.9589 - val_acc: 0.6266\n",
            "Epoch 8/8\n",
            "49000/49000 [==============================] - 2s 35us/step - loss: 0.7151 - acc: 0.7148 - val_loss: 0.9833 - val_acc: 0.6354\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f58633cccc0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75N0E1TN28qv"
      },
      "source": [
        "We decided to try the combination of epochs and batch size and try to see the most optimal combination to use. A more ideal approach in this case would be to use a grid search and go ahead. \n",
        "In this model, accuracy of training set is ~71% and one our testing set is 63%. It sill shows a bit of discrepancy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTYLY2ZEhAjf"
      },
      "source": [
        "We use the glove embeddings in our model to understand if using already available word embeddings could be helpful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWBNPmzNi5Ru"
      },
      "source": [
        "embeddings_index = dict()\n",
        "import io\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmNV8O8z3CUg"
      },
      "source": [
        "Here's how it knows how to match the tokenised words with the correct embeddings:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbEdPcgMlP3U"
      },
      "source": [
        "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmsOYJkr3WaB"
      },
      "source": [
        "And this is how it looks with the Embedding layer that uses the embeddings from Glove: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSIK-cs4nsx5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f4784c5c-bae9-44e3-a36f-781ef69f66b0"
      },
      "source": [
        "## create model\n",
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(vocabulary_size, 100, input_length=100, weights=[embedding_matrix], trainable=False))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
        "model_glove.add(MaxPooling1D(pool_size=5)) #originally 4\n",
        "model_glove.add(LSTM(100))\n",
        "model_glove.add(Dense(5, activation='softmax')) #originally sigmoid\n",
        "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "## Fit train data\n",
        "history=model_glove.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 30, batch_size=2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 49000 samples, validate on 21000 samples\n",
            "Epoch 1/30\n",
            "49000/49000 [==============================] - 4s 84us/step - loss: 1.2144 - acc: 0.5897 - val_loss: 1.1471 - val_acc: 0.6075\n",
            "Epoch 2/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.1200 - acc: 0.6167 - val_loss: 1.1203 - val_acc: 0.6083\n",
            "Epoch 3/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.0879 - acc: 0.6171 - val_loss: 1.1077 - val_acc: 0.6006\n",
            "Epoch 4/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.0578 - acc: 0.6176 - val_loss: 1.0479 - val_acc: 0.6104\n",
            "Epoch 5/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.0333 - acc: 0.6207 - val_loss: 1.0499 - val_acc: 0.6077\n",
            "Epoch 6/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.0189 - acc: 0.6216 - val_loss: 1.0403 - val_acc: 0.6040\n",
            "Epoch 7/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 1.0025 - acc: 0.6255 - val_loss: 1.0088 - val_acc: 0.6213\n",
            "Epoch 8/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9870 - acc: 0.6294 - val_loss: 0.9976 - val_acc: 0.6209\n",
            "Epoch 9/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9780 - acc: 0.6328 - val_loss: 0.9903 - val_acc: 0.6241\n",
            "Epoch 10/30\n",
            "49000/49000 [==============================] - 2s 32us/step - loss: 0.9672 - acc: 0.6351 - val_loss: 1.0158 - val_acc: 0.6094\n",
            "Epoch 11/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9610 - acc: 0.6377 - val_loss: 0.9892 - val_acc: 0.6198\n",
            "Epoch 12/30\n",
            "49000/49000 [==============================] - 2s 32us/step - loss: 0.9467 - acc: 0.6406 - val_loss: 0.9807 - val_acc: 0.6219\n",
            "Epoch 13/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9358 - acc: 0.6448 - val_loss: 0.9785 - val_acc: 0.6272\n",
            "Epoch 14/30\n",
            "49000/49000 [==============================] - 2s 32us/step - loss: 0.9267 - acc: 0.6478 - val_loss: 1.0276 - val_acc: 0.5995\n",
            "Epoch 15/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9223 - acc: 0.6491 - val_loss: 0.9708 - val_acc: 0.6295\n",
            "Epoch 16/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9149 - acc: 0.6502 - val_loss: 0.9660 - val_acc: 0.6319\n",
            "Epoch 17/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9138 - acc: 0.6531 - val_loss: 0.9647 - val_acc: 0.6319\n",
            "Epoch 18/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.9016 - acc: 0.6552 - val_loss: 0.9637 - val_acc: 0.6286\n",
            "Epoch 19/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8981 - acc: 0.6556 - val_loss: 0.9633 - val_acc: 0.6283\n",
            "Epoch 20/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8890 - acc: 0.6592 - val_loss: 0.9613 - val_acc: 0.6314\n",
            "Epoch 21/30\n",
            "49000/49000 [==============================] - 2s 32us/step - loss: 0.8891 - acc: 0.6603 - val_loss: 0.9739 - val_acc: 0.6298\n",
            "Epoch 22/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8873 - acc: 0.6596 - val_loss: 0.9643 - val_acc: 0.6285\n",
            "Epoch 23/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8759 - acc: 0.6616 - val_loss: 0.9713 - val_acc: 0.6230\n",
            "Epoch 24/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8698 - acc: 0.6643 - val_loss: 0.9892 - val_acc: 0.6131\n",
            "Epoch 25/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8700 - acc: 0.6647 - val_loss: 0.9577 - val_acc: 0.6350\n",
            "Epoch 26/30\n",
            "49000/49000 [==============================] - 1s 31us/step - loss: 0.8672 - acc: 0.6662 - val_loss: 0.9753 - val_acc: 0.6216\n",
            "Epoch 27/30\n",
            "49000/49000 [==============================] - 1s 31us/step - loss: 0.8598 - acc: 0.6684 - val_loss: 0.9670 - val_acc: 0.6268\n",
            "Epoch 28/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8525 - acc: 0.6710 - val_loss: 0.9661 - val_acc: 0.6289\n",
            "Epoch 29/30\n",
            "49000/49000 [==============================] - 2s 31us/step - loss: 0.8489 - acc: 0.6711 - val_loss: 0.9699 - val_acc: 0.6286\n",
            "Epoch 30/30\n",
            "49000/49000 [==============================] - 1s 31us/step - loss: 0.8421 - acc: 0.6755 - val_loss: 0.9593 - val_acc: 0.6303\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlzqmlUHppzt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "67ec3924-deac-4c3f-8dd2-56d11e127749"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.yscale('linear')\n",
        "plt.ylim((0.5,1))\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train set', 'test set'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKSiIiEhMQUFERGL/H1k9H9M4zPbvAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fi-FEpPSC8c"
      },
      "source": [
        "> In this case for the given epoch-batch size combination, Glove performs better than the previous models we built. We observe that even by using a high number of epochs the difference between training set and test set accuracy is ~ 4%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JRck629-Po0"
      },
      "source": [
        "## Building a benchmark model - Logistic regression - Review_Body\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSe9go1wSYju"
      },
      "source": [
        "Even though we have access to Glove, gensim and other embeddings, we may be able to build more simplistic models by using logistic regression and count vectorizer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OjVNCUk-T5j"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train2['review_body'],train2['star_rating'], test_size=0.30, random_state=9)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EelxasULsYyi"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "X_train = vectorizer.transform(X_train)\n",
        "X_test  = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEHAvkDTsfBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d016f5af-012f-480d-e71e-ecc817caa832"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "classifier = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=500)\n",
        "classifier.fit(X_train, y_train)\n",
        "score_lr = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score_lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6504333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHC9y9zgcReB"
      },
      "source": [
        "## Model Architecture and Accuracies - Review_Headline\n",
        "\n",
        "- We perfom the same analysis on review_headline to observe for any changes in the model parameters or accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmjWs713cZ31"
      },
      "source": [
        "#Uploading the original train dataset\n",
        "train = pd.read_csv('MC2.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEtccN02ceoC"
      },
      "source": [
        "#Since the review_title is not so computationally heavy to handle we go for 50% of the dataset rows from review_title.\n",
        "#10 percent of data\n",
        "\n",
        "train2 = train.sample(500000,random_state=123,axis=0)\n",
        "\n",
        "train2=train2[[\"star_rating\",\"review_headline\"]]\n",
        "train2.review_headline=train2.review_headline.astype(str)\n",
        "train2.star_rating=train2.star_rating.astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhgEmHuuc1Wa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9666c0d7-4005-42d2-d03c-8a008a34e543"
      },
      "source": [
        "#Splitting the data in train and test for our analyses like review_body.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train ,test = train_test_split(train2, test_size=0.30, random_state=9)\n",
        "\n",
        "train.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yteSAHrdBGY"
      },
      "source": [
        "Setting the conventional notations for dependent and independent variables. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOO-N13xc6dK"
      },
      "source": [
        "X=train['review_headline']\n",
        "y=train['star_rating']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3WqKX-rdQiX"
      },
      "source": [
        "#Cleaning the review_title data\n",
        "def clean_text(text):\n",
        "    \n",
        "    ## Remove punctuation\n",
        "    text = text.translate(string.punctuation)\n",
        "    \n",
        "    ## Convert words to lower case and split them\n",
        "    text = text.lower().split()\n",
        "    \n",
        "    ## Remove stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "    \n",
        "    text = \" \".join(text)\n",
        "    ## Clean the text\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text) \n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    ## Stemming\n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "    return text\n",
        "\n",
        "\n",
        "\n",
        "X= X.map(lambda x: clean_text(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cQpCMMJdT0e"
      },
      "source": [
        "#Representing the dummy encoding \n",
        "y=pd.get_dummies(y)\n",
        "y=y.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPhd5mzidZxI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4a93ea1b-3a8e-47d0-dad1-521cf9c10e9b"
      },
      "source": [
        "### Create sequence of word tokens, with maximum length=50. \n",
        "#Since it is review_title we believe a less length should work for review_titles\n",
        "\n",
        "vocabulary_size = 50000\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(sequences, maxlen=50)\n",
        "\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(350000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeaguVvddtNW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73bf5c80-3308-4b22-b284-1523c88192b8"
      },
      "source": [
        "#Splitting our 50% data into further training and test \n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.30, random_state=9)\n",
        "\n",
        "X_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(105000, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLwcDfU0fAcE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "7015ac31-cc06-49af-cd86-ef88ada26540"
      },
      "source": [
        "#Building a sequential model \n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(50000, 100, input_length=50))\n",
        "model.add(LSTM(200, dropout=0.2, recurrent_dropout=0.2)) \n",
        "model.add(Dense(5, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "## Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 25, batch_size=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 245000 samples, validate on 105000 samples\n",
            "Epoch 1/25\n",
            "245000/245000 [==============================] - 22s 89us/step - loss: 1.1827 - acc: 0.6062 - val_loss: 1.0855 - val_acc: 0.6153\n",
            "Epoch 2/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 1.0131 - acc: 0.6212 - val_loss: 0.9777 - val_acc: 0.6340\n",
            "Epoch 3/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.9347 - acc: 0.6463 - val_loss: 0.9399 - val_acc: 0.6494\n",
            "Epoch 4/25\n",
            "245000/245000 [==============================] - 20s 84us/step - loss: 0.8886 - acc: 0.6595 - val_loss: 0.9302 - val_acc: 0.6520\n",
            "Epoch 5/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.8630 - acc: 0.6687 - val_loss: 0.9345 - val_acc: 0.6526\n",
            "Epoch 6/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.8439 - acc: 0.6770 - val_loss: 0.9385 - val_acc: 0.6491\n",
            "Epoch 7/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.8264 - acc: 0.6850 - val_loss: 0.9505 - val_acc: 0.6472\n",
            "Epoch 8/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.8106 - acc: 0.6921 - val_loss: 0.9578 - val_acc: 0.6439\n",
            "Epoch 9/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7973 - acc: 0.6980 - val_loss: 0.9728 - val_acc: 0.6412\n",
            "Epoch 10/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7841 - acc: 0.7022 - val_loss: 0.9785 - val_acc: 0.6418\n",
            "Epoch 11/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7717 - acc: 0.7062 - val_loss: 0.9844 - val_acc: 0.6402\n",
            "Epoch 12/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7607 - acc: 0.7092 - val_loss: 0.9960 - val_acc: 0.6363\n",
            "Epoch 13/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7522 - acc: 0.7120 - val_loss: 1.0005 - val_acc: 0.6348\n",
            "Epoch 14/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7432 - acc: 0.7147 - val_loss: 1.0161 - val_acc: 0.6330\n",
            "Epoch 15/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7354 - acc: 0.7171 - val_loss: 1.0177 - val_acc: 0.6329\n",
            "Epoch 16/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7285 - acc: 0.7196 - val_loss: 1.0272 - val_acc: 0.6299\n",
            "Epoch 17/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7233 - acc: 0.7206 - val_loss: 1.0332 - val_acc: 0.6308\n",
            "Epoch 18/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7171 - acc: 0.7222 - val_loss: 1.0534 - val_acc: 0.6338\n",
            "Epoch 19/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7116 - acc: 0.7250 - val_loss: 1.0445 - val_acc: 0.6326\n",
            "Epoch 20/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7057 - acc: 0.7259 - val_loss: 1.0593 - val_acc: 0.6308\n",
            "Epoch 21/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.7003 - acc: 0.7279 - val_loss: 1.0644 - val_acc: 0.6301\n",
            "Epoch 22/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.6952 - acc: 0.7293 - val_loss: 1.0713 - val_acc: 0.6286\n",
            "Epoch 23/25\n",
            "245000/245000 [==============================] - 21s 84us/step - loss: 0.6905 - acc: 0.7313 - val_loss: 1.0815 - val_acc: 0.6309\n",
            "Epoch 24/25\n",
            "245000/245000 [==============================] - 20s 84us/step - loss: 0.6843 - acc: 0.7335 - val_loss: 1.0918 - val_acc: 0.6277\n",
            "Epoch 25/25\n",
            "245000/245000 [==============================] - 20s 83us/step - loss: 0.6799 - acc: 0.7352 - val_loss: 1.1013 - val_acc: 0.6280\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a956b7c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG50kT5Mi3dU"
      },
      "source": [
        "We built another deeper model with more layers and different parameters. We added 'relu' activation like the review_body model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycYtORr6jGfV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "c9e6775c-0d43-48c9-b1d6-14e7c9e6e7a8"
      },
      "source": [
        "def create_conv_model():\n",
        "    model_conv = Sequential()\n",
        "    model_conv.add(Embedding(vocabulary_size, 100, input_length=50))\n",
        "    model_conv.add(Dropout(0.8)) \n",
        "    model_conv.add(Conv1D(64, 5, activation='relu'))\n",
        "    model_conv.add(MaxPooling1D(pool_size=5)) #originally pool_size=4\n",
        "    model_conv.add(LSTM(100))\n",
        "    model_conv.add(Dense(5, activation='softmax'))\n",
        "    model_conv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model_conv\n",
        "model_conv = create_conv_model()\n",
        "model_conv.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 20, batch_size=5000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0630 08:48:29.078641 139685384861568 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 245000 samples, validate on 105000 samples\n",
            "Epoch 1/20\n",
            "245000/245000 [==============================] - 6s 25us/step - loss: 1.1972 - acc: 0.5960 - val_loss: 1.1804 - val_acc: 0.6153\n",
            "Epoch 2/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.1402 - acc: 0.6143 - val_loss: 1.1687 - val_acc: 0.6153\n",
            "Epoch 3/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.1073 - acc: 0.6143 - val_loss: 1.1374 - val_acc: 0.6153\n",
            "Epoch 4/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0729 - acc: 0.6143 - val_loss: 1.0970 - val_acc: 0.6157\n",
            "Epoch 5/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0443 - acc: 0.6189 - val_loss: 1.0590 - val_acc: 0.6222\n",
            "Epoch 6/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0180 - acc: 0.6234 - val_loss: 1.0278 - val_acc: 0.6311\n",
            "Epoch 7/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9933 - acc: 0.6322 - val_loss: 1.0175 - val_acc: 0.6356\n",
            "Epoch 8/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9790 - acc: 0.6359 - val_loss: 1.0182 - val_acc: 0.6350\n",
            "Epoch 9/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9697 - acc: 0.6381 - val_loss: 1.0086 - val_acc: 0.6350\n",
            "Epoch 10/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9622 - acc: 0.6401 - val_loss: 1.0078 - val_acc: 0.6352\n",
            "Epoch 11/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9560 - acc: 0.6421 - val_loss: 1.0093 - val_acc: 0.6336\n",
            "Epoch 12/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9503 - acc: 0.6444 - val_loss: 1.0060 - val_acc: 0.6331\n",
            "Epoch 13/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9460 - acc: 0.6462 - val_loss: 1.0116 - val_acc: 0.6324\n",
            "Epoch 14/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9419 - acc: 0.6485 - val_loss: 1.0054 - val_acc: 0.6315\n",
            "Epoch 15/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9368 - acc: 0.6496 - val_loss: 1.0079 - val_acc: 0.6303\n",
            "Epoch 16/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9332 - acc: 0.6518 - val_loss: 1.0010 - val_acc: 0.6343\n",
            "Epoch 17/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9289 - acc: 0.6538 - val_loss: 1.0020 - val_acc: 0.6324\n",
            "Epoch 18/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9254 - acc: 0.6559 - val_loss: 1.0036 - val_acc: 0.6323\n",
            "Epoch 19/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9219 - acc: 0.6562 - val_loss: 1.0041 - val_acc: 0.6315\n",
            "Epoch 20/20\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9188 - acc: 0.6588 - val_loss: 1.0036 - val_acc: 0.6317\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a8e6933c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrCd112ZlK7I"
      },
      "source": [
        "We use Glove embeddings for our review_headline analysis as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PyiCQpflKGw"
      },
      "source": [
        "#Loading the embeddings. \n",
        "embeddings_index = dict()\n",
        "import io\n",
        "f = open('glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXCmUFaalurN"
      },
      "source": [
        "#Matching embeddings with tokenized words from review_title\n",
        "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1:\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNpHzoivmJpH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "936d7323-e46e-47e3-b562-3ecf4e3ed8bd"
      },
      "source": [
        "## create model\n",
        "model_glove = Sequential()\n",
        "model_glove.add(Embedding(vocabulary_size, 100, input_length=50, weights=[embedding_matrix], trainable=False))\n",
        "model_glove.add(Dropout(0.2))\n",
        "model_glove.add(Conv1D(64, 5, activation='relu'))\n",
        "model_glove.add(MaxPooling1D(pool_size=5)) #originally 4\n",
        "model_glove.add(LSTM(100))\n",
        "model_glove.add(Dense(5, activation='softmax')) #originally sigmoid\n",
        "model_glove.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "## Fit train data\n",
        "history=model_glove.fit(X_train, y_train, validation_data=(X_val,y_val), epochs = 30, batch_size=2000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 245000 samples, validate on 105000 samples\n",
            "Epoch 1/30\n",
            "245000/245000 [==============================] - 7s 27us/step - loss: 1.1581 - acc: 0.6067 - val_loss: 1.0871 - val_acc: 0.6165\n",
            "Epoch 2/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0712 - acc: 0.6211 - val_loss: 1.0506 - val_acc: 0.6277\n",
            "Epoch 3/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0514 - acc: 0.6262 - val_loss: 1.0410 - val_acc: 0.6295\n",
            "Epoch 4/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0413 - acc: 0.6286 - val_loss: 1.0342 - val_acc: 0.6316\n",
            "Epoch 5/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0350 - acc: 0.6300 - val_loss: 1.0314 - val_acc: 0.6321\n",
            "Epoch 6/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0296 - acc: 0.6302 - val_loss: 1.0304 - val_acc: 0.6325\n",
            "Epoch 7/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0258 - acc: 0.6311 - val_loss: 1.0274 - val_acc: 0.6330\n",
            "Epoch 8/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0225 - acc: 0.6324 - val_loss: 1.0311 - val_acc: 0.6320\n",
            "Epoch 9/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0201 - acc: 0.6330 - val_loss: 1.0256 - val_acc: 0.6336\n",
            "Epoch 10/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0177 - acc: 0.6335 - val_loss: 1.0274 - val_acc: 0.6339\n",
            "Epoch 11/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0153 - acc: 0.6337 - val_loss: 1.0251 - val_acc: 0.6333\n",
            "Epoch 12/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0142 - acc: 0.6338 - val_loss: 1.0251 - val_acc: 0.6341\n",
            "Epoch 13/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0112 - acc: 0.6349 - val_loss: 1.0260 - val_acc: 0.6326\n",
            "Epoch 14/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0101 - acc: 0.6349 - val_loss: 1.0244 - val_acc: 0.6336\n",
            "Epoch 15/30\n",
            "245000/245000 [==============================] - 4s 18us/step - loss: 1.0078 - acc: 0.6349 - val_loss: 1.0257 - val_acc: 0.6335\n",
            "Epoch 16/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0074 - acc: 0.6356 - val_loss: 1.0261 - val_acc: 0.6322\n",
            "Epoch 17/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0067 - acc: 0.6356 - val_loss: 1.0255 - val_acc: 0.6335\n",
            "Epoch 18/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0052 - acc: 0.6362 - val_loss: 1.0250 - val_acc: 0.6335\n",
            "Epoch 19/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0037 - acc: 0.6364 - val_loss: 1.0257 - val_acc: 0.6341\n",
            "Epoch 20/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0027 - acc: 0.6372 - val_loss: 1.0263 - val_acc: 0.6329\n",
            "Epoch 21/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0022 - acc: 0.6365 - val_loss: 1.0258 - val_acc: 0.6323\n",
            "Epoch 22/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 1.0005 - acc: 0.6371 - val_loss: 1.0269 - val_acc: 0.6323\n",
            "Epoch 23/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9998 - acc: 0.6373 - val_loss: 1.0259 - val_acc: 0.6333\n",
            "Epoch 24/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9989 - acc: 0.6375 - val_loss: 1.0255 - val_acc: 0.6332\n",
            "Epoch 25/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9983 - acc: 0.6377 - val_loss: 1.0265 - val_acc: 0.6329\n",
            "Epoch 26/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9969 - acc: 0.6387 - val_loss: 1.0275 - val_acc: 0.6320\n",
            "Epoch 27/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9967 - acc: 0.6385 - val_loss: 1.0263 - val_acc: 0.6326\n",
            "Epoch 28/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9956 - acc: 0.6388 - val_loss: 1.0274 - val_acc: 0.6320\n",
            "Epoch 29/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9952 - acc: 0.6388 - val_loss: 1.0269 - val_acc: 0.6330\n",
            "Epoch 30/30\n",
            "245000/245000 [==============================] - 4s 17us/step - loss: 0.9949 - acc: 0.6393 - val_loss: 1.0287 - val_acc: 0.6321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTfcjnGFnuoG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "5577f259-cf1e-497f-b152-2ea7d62567c7"
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('model accuracy for review_title')\n",
        "plt.ylabel('accuracy')\n",
        "plt.yscale('linear')\n",
        "plt.ylim((0.5,1))\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train set', 'test set'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESTrsXmlu7Rm"
      },
      "source": [
        ">Comments on Glove Model.\n",
        "\n",
        "1. For glove, we see that there is barely any difference between the training set and testing set for our model on review_headline \n",
        "\n",
        "2. We believe that using external embeddings could help reduce the problem of overfitting\n",
        "\n",
        "3. A similar difference of less than ~4% was observed for glove model on review_body "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFnMHhaztMxk"
      },
      "source": [
        "## Benchmark model - Logistic Regression on Review Headline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-euEd5EctL-E"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train2['review_headline'],train2['star_rating'], test_size=0.30, random_state=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rpMngmJtYQ9"
      },
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(X_train)\n",
        "\n",
        "X_train = vectorizer.transform(X_train)\n",
        "X_test  = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYyTjpg4tbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6bd4aaa4-c1d4-4a3d-9473-883363072c7f"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "classifier = LogisticRegression(solver=\"lbfgs\", multi_class=\"multinomial\")\n",
        "classifier.fit(X_train, y_train)\n",
        "score_lr = classifier.score(X_test, y_test)\n",
        "\n",
        "print(\"Accuracy:\", score_lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAYWvD0pO_BW"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "**Accuracies and comparisons**\n",
        "\n",
        "**1. Analysis on Review_Body**  (vector-length 100):\n",
        "<br> Logistic Regression Accuracy : 0.65\n",
        "<br> Glove Neural Network : 0.63\n",
        "<br> 100-length vector representation model without relu : 0.60\n",
        "<br> 100-length vector representation model with relu: 0.635\n",
        "\n",
        "**2. Analysis on Review_headline** (vector-length 50 for non glove embeddings, 100 for glove embeddings):\n",
        "<br> Logistic Regression Accuracy : 0.659\n",
        "<br> Glove Neural Network : 0.63\n",
        "<br> 100-length vector representation model without relu : 0.62\n",
        "<br> 100-length vector representation model with relu: 0.63\n",
        "\n",
        "The main difference between both the above analyses is that:\n",
        "1. Various different combinations of epoch and batch size were tried\n",
        "2. For review_headline, a much higher dropout was used leading to significantly better results\n",
        "3. The difference between accuracies of training and test dataset for review_headline is much 'acceptable' than review_body models.\n",
        "\n",
        "Out of all three of them, logistic regression performs slightly better than our neural networks by just count tokenizer. However, these results could change if better activation functions, drop outs, grid size and epochs are defined. \n",
        "\n",
        "One of the main reasons for this according to us could be:\n",
        "- Due to computational issues, we decided to skip combining review_body and review_headline together\n",
        "- However, different variables like counting the total number of words in review_body and so on could be tried and achieved a higher output.\n",
        "- Combining the helpful_votes and total_votes\n",
        "\n",
        "<br> Depending on the business needs, time at hand, human and data resources available, it makes sense to decide whether to go for complex models or not. \n",
        "<br>Combining the entire text, other variables together might lead to higher accuracy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rahuipARxi47"
      },
      "source": [
        "## Data Upload Preparation\n",
        "> Applying this model on dataset to be uploaded basically these steps to be used for everything\n",
        "> We decided to go ahead with review title, since it is computationally less heavy and predictions can be applied on review title of unknown dataset given to us. \n",
        "> We have trained our model on ~350,000 rows for review_headline but only ~70,000 for review_body Hence, going ahead with review_headline makes more sense. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZmKQvK0xiVx"
      },
      "source": [
        "uploaddata=pd.read_csv('MC2test.csv')\n",
        "uploaddata.review_headline=uploaddata.review_headline.astype(str)\n",
        "\n",
        "uploadheadline=uploaddata['review_headline']\n",
        "uploadproductid=uploaddata['product_id']\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asLeWxrwdIw4"
      },
      "source": [
        "uploadheadline = uploadheadline.map(lambda x: clean_text(x))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgeAqhwQei6-"
      },
      "source": [
        "vocabulary_size = 50000\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(uploadheadline)\n",
        "sequences2 = tokenizer.texts_to_sequences(uploadheadline)\n",
        "UD = pad_sequences(sequences2, maxlen=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9FJvcXyfQNx"
      },
      "source": [
        "#Using Review Headline Model without glove since the accuracy is more or less the same. \n",
        "prediction = model_conv.predict_classes(UD)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GEsXyqMhaMbG"
      },
      "source": [
        "#Have to add +1 to predictions since Python recognizes it as 0 to 4 rather than 1 to 5\n",
        "upload_predictions= pd.DataFrame({'product_id':uploadproductid, 'star_rating':prediction+1})\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "upload_predictions.to_csv('MC2_Nexttopmodel_ReviewHeadline.csv')\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('MC2_Nexttopmodel_ReviewHeadline.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}